{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjAoFEmLn5Ww",
        "outputId": "5ff1881e-9fbd-4157-88fe-0cf72497ce36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ],
      "source": [
        "# to import files from googledrive\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "drive.mount('/content/gdrive/', force_remount=True)\n",
        "sample_size_dir = 'gdrive/My Drive/Sample size for NLP'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdt2omCBo-Ql"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_UcETQxn_p9",
        "outputId": "63bab4e7-9e65-4d86-d151-2b1925552ef5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers datasets scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jk4-mGWZoYJo"
      },
      "outputs": [],
      "source": [
        "!pip install -q accelerate -U\n",
        "!pip install -q transformers[torch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kT41j5poobq8"
      },
      "outputs": [],
      "source": [
        "## Testing on imdb data for code checks - Ignore if using actual data ##\n",
        "\n",
        "'''\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load a dataset, for example, the IMDb dataset\n",
        "dataset = load_dataset('imdb')\n",
        "\n",
        "### Use only a portion of the data ###\n",
        "\n",
        "# Define the proportion of the dataset to use\n",
        "train_size = 0.1\n",
        "test_size = 0.1t\n",
        "\n",
        "# Shuffle and select a subset of the dataset\n",
        "train_dataset = dataset['train'].shuffle(seed=42).select(range(int(train_size * len(dataset['train']))))\n",
        "test_dataset = dataset['test'].shuffle(seed=42).select(range(int(test_size * len(dataset['test']))))\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuZBqLPEysJI"
      },
      "outputs": [],
      "source": [
        "df1 = pd.read_csv(sample_size_dir+'/data_Kai/new_6000/4019_merged_report_6000_without.csv')\n",
        "df2 = pd.read_csv(sample_size_dir+'/data_Kai/new_6000/4019_merged_report_6000_with.csv')\n",
        "\n",
        "# Randomly sample 500 rows from each DataFrame\n",
        "sampled_df1 = df1.sample(n=500, random_state=42)\n",
        "sampled_df2 = df2.sample(n=500, random_state=42)\n",
        "\n",
        "# Combine the sampled data\n",
        "combined_df = pd.concat([sampled_df1, sampled_df2])\n",
        "\n",
        "# Add a column 'label' and set it to 1 if 'ICD9_CODE' is 4019, else 0\n",
        "combined_df['label'] = combined_df['ICD9_CODE'].apply(lambda x: 1 if x == 4019 else 0)\n",
        "\n",
        "# Shuffle the combined DataFrame\n",
        "shuffled_combined_df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Save the combined data to a new CSV file\n",
        "combined_df.to_csv(sample_size_dir+'/data_jaya/4019_combined_data_1000.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0V-nMgEHz0IJ"
      },
      "outputs": [],
      "source": [
        "# Separate the DataFrame into two DataFrames based on the label\n",
        "df_label_1 = combined_df[combined_df['label'] == 1]\n",
        "df_label_0 = combined_df[combined_df['label'] == 0]\n",
        "\n",
        "# Split each DataFrame into train and test sets\n",
        "train_label_1, test_label_1 = train_test_split(df_label_1, test_size=0.2, random_state=42)\n",
        "train_label_0, test_label_0 = train_test_split(df_label_0, test_size=0.2, random_state=42)\n",
        "\n",
        "# Concatenate the train and test sets\n",
        "train_dataset = pd.concat([train_label_1, train_label_0]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "test_dataset = pd.concat([test_label_1, test_label_0]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Save the subsets to new CSV files\n",
        "train_dataset.to_csv(sample_size_dir+'/data_jaya/4019_train_dataset.csv', index=False)\n",
        "test_dataset.to_csv(sample_size_dir+'/data_jaya/4019_test_dataset.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wo4m-Ohc04LE",
        "outputId": "2ba7e0ae-df3a-442c-af16-bc1c2eec09d3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label\n",
              "0    0.5\n",
              "1    0.5\n",
              "Name: proportion, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "train_dataset['label'].value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LJ0bQv2073T",
        "outputId": "7040efc4-bce8-4da4-f101-7d2f8128a642"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label\n",
              "1    0.5\n",
              "0    0.5\n",
              "Name: proportion, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "test_dataset['label'].value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "cCp-gmlWosl5",
        "outputId": "f5a18f84-894e-4873-e2f3-aa680160c7f7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n# for imbd\\n#def tokenize_function(examples):\\n#    return tokenizer(examples[\\'TEXT\\'], padding=\"max_length\", truncation=True) # \\'text\\' if using imdb\\n\\n# Define a function to tokenize the text - for dataframe\\ndef tokenize_function(text):\\n    return tokenizer(text, padding=\"max_length\", truncation=True)\\n\\n### If using the entire dataset ###\\n\\n# tokenized_datasets = dataset.map(tokenize_function, batched=True)\\n# tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\\n# tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\\n# tokenized_datasets.set_format(\"torch\")\\n\\n### If using portion of dataset ###\\n\\n# Tokenize the datasets - for dataframe\\ntrain_dataset[\\'input_ids\\'] = train_dataset[\\'TEXT\\'].apply(lambda x: tokenize_function(x)[\\'input_ids\\'])\\ntrain_dataset[\\'attention_mask\\'] = train_dataset[\\'TEXT\\'].apply(lambda x: tokenize_function(x)[\\'attention_mask\\'])\\n\\ntest_dataset[\\'input_ids\\'] = test_dataset[\\'TEXT\\'].apply(lambda x: tokenize_function(x)[\\'input_ids\\'])\\ntest_dataset[\\'attention_mask\\'] = test_dataset[\\'TEXT\\'].apply(lambda x: tokenize_function(x)[\\'attention_mask\\'])\\n\\n# Tokenize the datasets - for huggingface data like imdb\\n# tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\\n# tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\\n\\n# Remove unnecessary columns and set format to PyTorch\\n#tokenized_train_dataset = train_dataset.drop([\"TEXT\"]) # \\'text\\' if using imdb and remove_columns instead of drop\\n#tokenized_test_dataset = test_dataset.drop([\"TEXT\"]) # \\'text\\' if using imdb and remove_columns instead of drop\\n\\ntokenized_train_dataset = train_dataset\\ntokenized_test_dataset = test_dataset\\ntrain_dataset = train_dataset.rename(columns={\\'label\\': \\'labels\\'})\\ntest_dataset = test_dataset.rename(columns={\\'label\\': \\'labels\\'})\\n#tokenized_train_dataset = tokenized_train_dataset.rename_column(\"label\", \"labels\") #rename_column if using imdb\\n#tokenized_test_dataset = tokenized_test_dataset.rename_column(\"label\", \"labels\") #rename_column if using imdb\\n\\n# Now train_dataset and test_dataset have columns: \\'TEXT\\', \\'input_ids\\', \\'attention_mask\\', \\'labels\\'\\n\\n# Convert to PyTorch tensors (assuming labels are already numeric)\\ntrain_input_ids = torch.tensor(train_dataset[\\'input_ids\\'].tolist(), dtype=torch.long)\\ntrain_attention_mask = torch.tensor(train_dataset[\\'attention_mask\\'].tolist(), dtype=torch.long)\\ntrain_labels = torch.tensor(train_dataset[\\'labels\\'].tolist(), dtype=torch.long)\\n\\ntest_input_ids = torch.tensor(test_dataset[\\'input_ids\\'].tolist(), dtype=torch.long)\\ntest_attention_mask = torch.tensor(test_dataset[\\'attention_mask\\'].tolist(), dtype=torch.long)\\ntest_labels = torch.tensor(test_dataset[\\'labels\\'].tolist(), dtype=torch.long)\\n\\n# Create TensorDatasets\\ntokenized_train_dataset = TensorDataset(train_input_ids, train_attention_mask, train_labels)\\ntokenized_test_dataset = TensorDataset(test_input_ids, test_attention_mask, test_labels)\\n\\n\\n# Example of how to create DataLoader (you can adjust batch_size and shuffle as needed)\\n#tokenized_test_dataloader = torch.utils.data.DataLoader(tokenized_test_dataset, batch_size=16, shuffle=False)\\n#tokenized_train_dataloader = torch.utils.data.DataLoader(tokenized_train_dataset, batch_size=16, shuffle=True)\\n\\n\\n#tokenized_test_dataset.set_format(\"torch\")\\n#tokenized_train_dataset.set_format(\"torch\")\\n'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "\n",
        "# Function to tokenize the text\n",
        "def tokenize_function(text):\n",
        "    return tokenizer(text, padding=\"max_length\", truncation=True)\n",
        "\n",
        "# Assuming train_dataset and test_dataset are pandas DataFrames with columns 'TEXT' and 'label'\n",
        "# Tokenize the datasets\n",
        "train_dataset['input_ids'] = train_dataset['TEXT'].apply(lambda x: tokenize_function(x)['input_ids'])\n",
        "train_dataset['attention_mask'] = train_dataset['TEXT'].apply(lambda x: tokenize_function(x)['attention_mask'])\n",
        "\n",
        "test_dataset['input_ids'] = test_dataset['TEXT'].apply(lambda x: tokenize_function(x)['input_ids'])\n",
        "test_dataset['attention_mask'] = test_dataset['TEXT'].apply(lambda x: tokenize_function(x)['attention_mask'])\n",
        "\n",
        "# Rename the label column to 'labels' (assuming 'label' is the original column name)\n",
        "train_dataset = train_dataset.rename(columns={'label': 'labels'})\n",
        "test_dataset = test_dataset.rename(columns={'label': 'labels'})\n",
        "\n",
        "# Convert to lists of dictionaries for Trainer compatibility\n",
        "train_examples = [{'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels} for input_ids, attention_mask, labels in zip(train_dataset['input_ids'], train_dataset['attention_mask'], train_dataset['labels'])]\n",
        "test_examples = [{'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels} for input_ids, attention_mask, labels in zip(test_dataset['input_ids'], test_dataset['attention_mask'], test_dataset['labels'])]\n",
        "\n",
        "\n",
        "'''\n",
        "# for imbd\n",
        "#def tokenize_function(examples):\n",
        "#    return tokenizer(examples['TEXT'], padding=\"max_length\", truncation=True) # 'text' if using imdb\n",
        "\n",
        "# Define a function to tokenize the text - for dataframe\n",
        "def tokenize_function(text):\n",
        "    return tokenizer(text, padding=\"max_length\", truncation=True)\n",
        "\n",
        "### If using the entire dataset ###\n",
        "\n",
        "# tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "# tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
        "# tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
        "# tokenized_datasets.set_format(\"torch\")\n",
        "\n",
        "### If using portion of dataset ###\n",
        "\n",
        "# Tokenize the datasets - for dataframe\n",
        "train_dataset['input_ids'] = train_dataset['TEXT'].apply(lambda x: tokenize_function(x)['input_ids'])\n",
        "train_dataset['attention_mask'] = train_dataset['TEXT'].apply(lambda x: tokenize_function(x)['attention_mask'])\n",
        "\n",
        "test_dataset['input_ids'] = test_dataset['TEXT'].apply(lambda x: tokenize_function(x)['input_ids'])\n",
        "test_dataset['attention_mask'] = test_dataset['TEXT'].apply(lambda x: tokenize_function(x)['attention_mask'])\n",
        "\n",
        "# Tokenize the datasets - for huggingface data like imdb\n",
        "# tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "# tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Remove unnecessary columns and set format to PyTorch\n",
        "#tokenized_train_dataset = train_dataset.drop([\"TEXT\"]) # 'text' if using imdb and remove_columns instead of drop\n",
        "#tokenized_test_dataset = test_dataset.drop([\"TEXT\"]) # 'text' if using imdb and remove_columns instead of drop\n",
        "\n",
        "tokenized_train_dataset = train_dataset\n",
        "tokenized_test_dataset = test_dataset\n",
        "train_dataset = train_dataset.rename(columns={'label': 'labels'})\n",
        "test_dataset = test_dataset.rename(columns={'label': 'labels'})\n",
        "#tokenized_train_dataset = tokenized_train_dataset.rename_column(\"label\", \"labels\") #rename_column if using imdb\n",
        "#tokenized_test_dataset = tokenized_test_dataset.rename_column(\"label\", \"labels\") #rename_column if using imdb\n",
        "\n",
        "# Now train_dataset and test_dataset have columns: 'TEXT', 'input_ids', 'attention_mask', 'labels'\n",
        "\n",
        "# Convert to PyTorch tensors (assuming labels are already numeric)\n",
        "train_input_ids = torch.tensor(train_dataset['input_ids'].tolist(), dtype=torch.long)\n",
        "train_attention_mask = torch.tensor(train_dataset['attention_mask'].tolist(), dtype=torch.long)\n",
        "train_labels = torch.tensor(train_dataset['labels'].tolist(), dtype=torch.long)\n",
        "\n",
        "test_input_ids = torch.tensor(test_dataset['input_ids'].tolist(), dtype=torch.long)\n",
        "test_attention_mask = torch.tensor(test_dataset['attention_mask'].tolist(), dtype=torch.long)\n",
        "test_labels = torch.tensor(test_dataset['labels'].tolist(), dtype=torch.long)\n",
        "\n",
        "# Create TensorDatasets\n",
        "tokenized_train_dataset = TensorDataset(train_input_ids, train_attention_mask, train_labels)\n",
        "tokenized_test_dataset = TensorDataset(test_input_ids, test_attention_mask, test_labels)\n",
        "\n",
        "\n",
        "# Example of how to create DataLoader (you can adjust batch_size and shuffle as needed)\n",
        "#tokenized_test_dataloader = torch.utils.data.DataLoader(tokenized_test_dataset, batch_size=16, shuffle=False)\n",
        "#tokenized_train_dataloader = torch.utils.data.DataLoader(tokenized_train_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "\n",
        "#tokenized_test_dataset.set_format(\"torch\")\n",
        "#tokenized_train_dataset.set_format(\"torch\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ss85pauv4oqn"
      },
      "outputs": [],
      "source": [
        "# To try truncating from the beginning of the text rather than the end\n",
        "\n",
        "'''\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Define the maximum length for BERT\n",
        "max_length = 512  # Adjust based on your BERT model\n",
        "\n",
        "# Function to truncate the beginning of the text\n",
        "def truncate_text(text, max_length):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    if len(tokens) > max_length:\n",
        "        tokens = tokens[-max_length:]\n",
        "    return tokenizer.convert_tokens_to_string(tokens)\n",
        "\n",
        "# Define a function to tokenize the text\n",
        "def tokenize_function(text):\n",
        "    truncated_text = truncate_text(text, max_length)\n",
        "    return tokenizer(truncated_text, padding=\"max_length\", truncation=True, max_length=max_length)\n",
        "\n",
        "# Tokenize the datasets\n",
        "train_dataset['input_ids'] = train_dataset['TEXT'].apply(lambda x: tokenize_function(x)['input_ids'])\n",
        "train_dataset['attention_mask'] = train_dataset['TEXT'].apply(lambda x: tokenize_function(x)['attention_mask'])\n",
        "\n",
        "test_dataset['input_ids'] = test_dataset['TEXT'].apply(lambda x: tokenize_function(x)['input_ids'])\n",
        "test_dataset['attention_mask'] = test_dataset['TEXT'].apply(lambda x: tokenize_function(x)['attention_mask'])\n",
        "\n",
        "# Drop the original text column\n",
        "train_dataset = train_dataset.drop(columns=['TEXT'])\n",
        "test_dataset = test_dataset.drop(columns=['TEXT'])\n",
        "\n",
        "# Rename the label column to labels\n",
        "train_dataset = train_dataset.rename(columns={'label': 'labels'})\n",
        "test_dataset = test_dataset.rename(columns={'label': 'labels'})\n",
        "\n",
        "# Define a custom PyTorch Dataset - for compatibility with PyTorch DataLoader so it can be used in batches, and is more memory efficient for parallel loading and processing\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.labels = df['labels'].values\n",
        "        self.input_ids = list(df['input_ids'].values)\n",
        "        self.attention_mask = list(df['attention_mask'].values)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {\n",
        "            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(self.attention_mask[idx], dtype=torch.long),\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "        return item\n",
        "\n",
        "# Convert the DataFrame to a PyTorch Dataset\n",
        "tokenized_train_dataset = CustomDataset(train_dataset)\n",
        "tokenized_test_dataset = CustomDataset(test_dataset)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyk9eRLkotc9"
      },
      "outputs": [],
      "source": [
        "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "def model_init():\n",
        "    return BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-lqlIyhovVK"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Define function to train the model\n",
        "def train_model(params):\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=sample_size_dir+'/data_jaya/results',\n",
        "        learning_rate=params['learning_rate'],\n",
        "        per_device_train_batch_size=params['per_device_train_batch_size'],\n",
        "        num_train_epochs=params['num_train_epochs'],\n",
        "        warmup_steps=500,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir='./logs',\n",
        "        logging_steps=10,\n",
        "        evaluation_strategy=\"epoch\"\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model_init=model_init,\n",
        "        args=training_args,\n",
        "        train_dataset=train_examples,\n",
        "        eval_dataset=test_examples\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    eval_result = trainer.evaluate()\n",
        "    return eval_result['eval_loss']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RFDkFXJZxnkx",
        "outputId": "d8e85335-5c02-4ac5-c891-f9f947460921"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'gdrive/My Drive/Sample size for NLP/data_jaya/results'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output_dir= sample_size_dir+'/data_jaya/results'\n",
        "output_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99hJmV5vowq1"
      },
      "source": [
        "# Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fnO48awozpk"
      },
      "outputs": [],
      "source": [
        "# Define the grid of hyperparameters\n",
        "param_grid = {\n",
        "    'learning_rate': [5e-5, 3e-5, 2e-5],\n",
        "    'per_device_train_batch_size': [8, 16],\n",
        "    'num_train_epochs': [3, 4]\n",
        "}\n",
        "\n",
        "# Convert param_grid to a list of parameter dictionaries\n",
        "param_list = list(ParameterGrid(param_grid))\n",
        "\n",
        "# List to store results\n",
        "results = []\n",
        "\n",
        "# Conduct the grid search\n",
        "best_loss = float('inf')\n",
        "best_params = None\n",
        "\n",
        "for params in param_list:\n",
        "    eval_loss = train_model(params)\n",
        "    results.append({'params': params, 'eval_loss': eval_loss})\n",
        "\n",
        "    if eval_loss < best_loss:\n",
        "        best_loss = eval_loss\n",
        "        best_params = params\n",
        "\n",
        "print(\"Best params:\", best_params)\n",
        "print(\"Best loss:\", best_loss)\n",
        "\n",
        "# Convert results to DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Save DataFrame to a CSV file\n",
        "results_df.to_csv(sample_size_dir+'/data_jaya/grid_search_results_53081.csv', index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGNOxpPPmMvM",
        "outputId": "876b5e13-d0c5-4ff1-9de8-2b6b4c5f69bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params: {'learning_rate': 5e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 16}\n",
            "Best loss: 0.26487576961517334\n"
          ]
        }
      ],
      "source": [
        "print(\"Best params:\", best_params)\n",
        "print(\"Best loss:\", best_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMnFXunHnBjJ"
      },
      "outputs": [],
      "source": [
        "results_df = pd.read_csv(sample_size_dir+'/data_jaya/grid_search_results_53081.csv')\n",
        "results_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elFIPyCKnDcw"
      },
      "outputs": [],
      "source": [
        "results_df.to_csv(sample_size_dir+'/data_jaya/grid_search_results.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQD-ItF7pf2D"
      },
      "source": [
        "# Bayesian Optimisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX9rLq-K5rBd"
      },
      "source": [
        "Bayesian optimization is a strategy for optimizing black-box functions that are expensive to evaluate. It is particularly useful in scenarios where each function evaluation is costly, such as hyperparameter tuning in machine learning models. It fits a Gaussian Process to model the relationship between hyperparameters and performance. And then uses the Gaussian Process to predict performance and compute the acquisition function to identify the next hyperparameter set to try.\n",
        "\n",
        "A Gaussian function, also known as a Gaussian distribution or normal distribution, is a symmetric, bell-shaped function that is widely used in statistics, probability theory, and various scientific fields. It describes how values of a variable are distributed, with most values clustering around a central peak and probabilities tapering off as you move away from the center."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab79qrMuphC_",
        "outputId": "91a08d0b-b5a3-4e88-b5b3-e72d485b68d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/380.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.3/380.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_0nIL0lqLvV"
      },
      "outputs": [],
      "source": [
        "def model_init():\n",
        "    return BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1Xzjl1kqLsu"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "\n",
        "# List to store results\n",
        "results = []\n",
        "\n",
        "def objective(trial):\n",
        "    # Define the hyperparameter search space\n",
        "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-5)\n",
        "    per_device_train_batch_size = trial.suggest_categorical('per_device_train_batch_size', [8, 16])\n",
        "    num_train_epochs = trial.suggest_int('num_train_epochs', 2, 4)\n",
        "\n",
        "    # Define the training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',\n",
        "        learning_rate=learning_rate,\n",
        "        per_device_train_batch_size=per_device_train_batch_size,\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        warmup_steps=500,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir='./logs',\n",
        "        logging_steps=10,\n",
        "        evaluation_strategy=\"epoch\"    # evaluate at the end of each epoch\n",
        "    )\n",
        "\n",
        "    # Define the trainer\n",
        "    trainer = Trainer(\n",
        "        model_init=model_init,\n",
        "        args=training_args,\n",
        "        train_dataset=train_examples,\n",
        "        eval_dataset=test_examples\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "    eval_result = trainer.evaluate()\n",
        "    return eval_result['eval_loss']\n",
        "\n",
        "    # Append trial results to the list\n",
        "    results.append({\n",
        "        'learning_rate': learning_rate,\n",
        "        'per_device_train_batch_size': per_device_train_batch_size,\n",
        "        'num_train_epochs': num_train_epochs,\n",
        "        'eval_loss': eval_result['eval_loss']\n",
        "    })\n",
        "\n",
        "    return eval_result['eval_loss']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TIvS4vc0qLqp"
      },
      "outputs": [],
      "source": [
        "# Create a study and optimize the objective function\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "\n",
        "print(f\"  Value: {trial.value}\")\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(f\"    {key}: {value}\")\n",
        "\n",
        "# Convert results to DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Save DataFrame to a CSV file\n",
        "results_df.to_csv(sample_size_dir+'/data_jaya/optuna_results_53081.csv', index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BG8iiTwGqLoC",
        "outputId": "84385087-192f-4ca0-d534-655480fd0ffb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best trial:\n",
            "  Value: 0.6818121075630188\n",
            "  Params: \n",
            "    learning_rate: 3.0819032743298975e-05\n",
            "    per_device_train_batch_size: 8\n",
            "    num_train_epochs: 3\n"
          ]
        }
      ],
      "source": [
        "# Get the best hyperparameters\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "\n",
        "print(f\"  Value: {trial.value}\")\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(f\"    {key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLYSAX_6qLlN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}